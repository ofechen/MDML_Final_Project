---
title: "MDML_Fianl_Project_YJ_FT_OC"
output: html_notebook
author: Yeonji Jung, Frankie Tam, Ofer Chen
---
#Descriptions

##BACKGROUND
Hints are an important learning aid. But students may have a tendency to use hints inappropriately. Offering hints indiscriminately can result in poor learning performance. For teachers to decide whether to show a hint or scaffolding to students in the further iteration, a prediction model on student hint-taking behaviors can be used to make adaptive decisions on whether to withhold or provide hints.

##DATA 
The dataset is the ASSISTments' skill builder problem sets data that gathered in the school year 2009 and 2010. ASSISTments is an online tutoring system that teachers can use to teach middle-school level Mathematics and identify student activities solving exercises on the system. 

The dataset consists of log data of student activities including the chronological order of attempts (order_id), assignment id, problem id, user id, whether the first attempt is correct (correct), whether the number of attempts on the problem (attempt_count), the number of using hints on the problem (hint_count), whether or not the student asks for all hints (bottom_hint), the time in milliseconds for the student to complete the problem (overlap_time), time between start time and first student action (first_response_time), whether the first action is attempt, using hint, or using scaffolding (first_action). Each row represents a single problem of each student.

The dataset has the following features:
(a) questions are based on one specific skill, a question can have multiple skill taggings. 
(b) students must answer three questions correct in a row to complete the assignment
(c) if a student uses the tutoring ("hint" or "scaffolding"), the question will be marked incorrect
(d) students will know immediately if they answered the question correctly
(e) scaffolding means whether student asks for the problem to be broken into steps
(f) The bottom out hint is the last hint for a problem and will generally contain the problemâ€™s answer.

The raw dataset is available at https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-data/skill-builder-data-2009-2010. 

##PURPOSE
Building a prediction model on student actions on next question
(a) whether students will take a hint at first attempt (first_action)
(b) whether students will take a bottom-out hint (bottom_hint)
(c) whether students will answer correctly without hints (correct)

##APPROACH 
We'll build three different models predicting student actions on next question: (a) hint-taking, (b) bottom-out hint-taking, (c) correct answers. For each part, we select features, split train and test set, fit two models of logistic regression and random forest, test it with test dataset, and choose between 2 models calculating AUC. (why you think this is a reasonable approach to take). 


#Part A: Import and Clean the Data
##Import packages and data
```{r}
#load packages
library(tidyverse)
library(randomForest)
library(doParallel)
library(foreach)
library(car)
library(ROCR)
#import the data
df_raw <- read.csv("skill_builder_data.csv")
```

##Clean the data
```{r}
#filter the following values
##("original") scaffolding problems (0); this is not relevant to this project
##("first_action") empty values; student clicked on the problem but did nothing else
##("first_action") scaffolding values; scaffolding opportunities varied across problem sets
##("answer_type") open_response; this type of answers always marked correct
##("ms_first_response") negative values and zero
##("overlap_time") negative values and zero
df_clean <- df_raw %>% 
  filter(original==1, first_action!=2, !is.na(first_action), 
         answer_type!="open_response", ms_first_response > 0, overlap_time > 0) %>%
  rename(problem_type = type)
#revalue the following columns:
##("bottom_hint") replace empty values into 0; it means the student did not ask for a hint
df_clean$bottom_hint[is.na(df_clean$bottom_hint)] <- 0
```

#Part B: Hint-Taking Prediction
##Feature engineering; make historical features
```{r}
#use the latest order id as reference
df_last <- df_clean %>% group_by(user_id) %>% arrange(order_id) %>% slice(n())
df_hist <- merge(df_last, df_clean[c('user_id', 'order_id', 'problem_id', 'correct', 'hint_count', 'hint_total', 'bottom_hint', 'ms_first_response', 'attempt_count')], by = 'user_id', all.x = TRUE, allow.cartesian=TRUE)
df_hist <- df_hist %>% filter(order_id.y < order_id.x)
df_hist <- df_hist %>% group_by(user_id, order_id.x) %>% 
  summarise(prev_total_correct = sum(correct.y),
            prev_num_problem = length(unique(problem_id.y)),
            prev_percent_correct=prev_total_correct/prev_num_problem,
            prev_questions_with_hints = sum(hint_total.y>0),
            prev_total_hints = sum(hint_total.y),
            prev_avg_hints_per_problem = prev_total_hints/prev_questions_with_hints,
            prev_problem_requested_hint = sum(hint_count.y>0),
            prev_percent_problem_requested_hint = prev_problem_requested_hint/prev_num_problem,
            prev_avg_response_time = mean(ms_first_response.y),
            prev_total_bottom_hint = sum(bottom_hint.y == 1),
            prev_percent_bottom_hint = prev_total_bottom_hint/prev_questions_with_hints,
            prev_avg_attempt_count = mean(attempt_count.y)) 
df_hist <- replace(df_hist, is.na(df_hist), 0)
df_fin <-  df_clean %>% left_join(df_hist)

# Maybe we should consider combining all skill ids in one variable and skill names in another

#convert to factors
df_fin <- df_fin %>%
  mutate_at(vars(first_action,correct,bottom_hint,problem_id,teacher_id, school_id),funs(as.factor))
```

##Create a training Set / test Set
```{r}
#randomly shuffle the data
set.seed(1314)
df_fin <- df_fin %>% slice(sample(1:n()))
#create train_half and test_half
splitsize = floor(nrow(df_fin)/2)
train <- df_fin %>% slice(1:splitsize)
test <- df_fin %>% slice(splitsize+1:n())
```

##Fit a logistic model on train and compute AUC on test set
```{r}
#fit a random forest model on train
lg_hint <- glm(first_action ~ tutor_mode + answer_type + prev_total_correct + 
                 prev_percent_correct + prev_problem_requested_hint + 
                 prev_percent_problem_requested_hint + prev_avg_response_time + 
                 prev_total_bottom_hint + prev_percent_bottom_hint + prev_avg_attempt_count, 
               data=train, family="binomial")

#compute AUC of this model on the test dataset  
test$lg.hint.pred.prob <- predict(lg_hint,newdata=test,type='response')
test.lg.hint.pred <- prediction(test$lg.hint.pred.prob, test$first_action)
test.lg.hint.perf <- performance(test.lg.hint.pred, "auc")
cat('the auc score is ', 100*test.lg.hint.perf@y.values[[1]], "\n")  

#calculate the five smallest and five largest coefficients 
lg_hint_coef <- summary(lg_hint)[["coefficients"]]
lg_hint_coef[order(lg_hint_coef[ , 1]), ]   
```
Before fitting a model, we dropped some features (which we hoped to embed in the model); 
  problem_type = only 1 factor 
  problem_id = more than 53 factors
  teacher_id = more than 53 factors
  
The AUC of the logistic model predicting hint-taking on the test dataset is 84.2032

The five smallest and five largest coefficients in this model
  Smallest: answer_typechoose_1 < answer_typechoose_n < prev_percent_correct <
            prev_total_bottom_hint < prev_total_correct 
  Largest:  tutor_modetutor > prev_percent_bottom_hint > answer_typefill_in_1 > 
            prev_percent_problem_requested_hint > prev_problem_requested_hint
### The mode being tutor is the strongest predictor because no one has taken a hint during a test (I verified that), it seems like students prefer to try answering and not get hints when they know it will be counted as a mistake anyway. Maybe we should drop test questions altogether, it will definitely hurt the prediction but I think the model makes more sense that way.
  
The features we decided to drop: 
  (a) prev_problem_requested_hint, (b) prev_percent_correct, (c) prev_total_bottom_hint
  These features are somewhat similar in content respectively with 
  (a1) prev_percent_problem_requested_hint, (b1) prev_total_correct, and (c1) prev_percent_bottom_hint, 
  and each had relatively low coefficient than its compared feature. 
  
##Drop the predictors and refit the logistic model 
```{r}
#refit a random forest model on train
re_lg_hint <- glm(first_action ~ tutor_mode + answer_type + prev_total_correct + 
                 prev_percent_problem_requested_hint + prev_avg_response_time + 
                 prev_percent_bottom_hint + prev_avg_attempt_count, 
               data=train, family="binomial")

#compute AUC of this model on the test dataset  
test$re.lg.hint.pred.prob <- predict(re_lg_hint,newdata=test,type='response')
test.re.lg.hint.pred <- prediction(test$re.lg.hint.pred.prob, test$first_action)
test.re.lg.hint.perf <- performance(test.re.lg.hint.pred, "auc")
cat('the auc score is ', 100*test.re.lg.hint.perf@y.values[[1]], "\n")  

#calculate the five smallest and five largest coefficients 
re_lg_hint_coef <- summary(re_lg_hint)[["coefficients"]]
re_lg_hint_coef[order(re_lg_hint_coef[ , 1]), ]   
```

The AUC of the logistic model predicting hint-taking on the test dataset is 82.84269 (*decreased from 84.2)

The smallest and five largest coefficients in this model
  Smallest: answer_typechoose_1 < answer_typechoose_n < prev_percent_problem_requested_hint < 
            prev_total_correct 
  Largest:  tutor_modetutor > prev_percent_bottom_hint > answer_typefill_in_1 > 
            prev_avg_attempt_count > prev_avg_response_time
            
### Maybe add some explanation as to why we decided to stay without the 3 variables dropped if the AUC score is lower? maybe for easier interpretability?
            
```{r}
#fit a random forest model on train
rf_hint <- randomForest(first_action ~ tutor_mode + answer_type + prev_total_correct + 
                 prev_percent_problem_requested_hint + prev_avg_response_time + 
                 prev_percent_bottom_hint + prev_avg_attempt_count, data=train, ntree=200, 
                 na.action=na.omit)

#compute AUC of this model on the test dataset  
test$rf.hint.pred.prob <- predict(rf_hint,newdata=test,type='prob')[,2]
test.rf.hint.pred <- prediction(test$rf.hint.pred.prob, test$first_action)
test.rf.hint.perf <- performance(test.rf.hint.pred, "auc")
cat('the auc score is ', 100*test.rf.hint.perf@y.values[[1]], "\n") 
```

The computed auc score is 70.20511. *How does the AUC of the random forest compare with the AUC of the logistic regression model?

```{r}
#compares the performance of two models using the plots
##create a performance plot
test.1 <- test %>% mutate(first_action = as.integer(first_action)-1)
plot.data.rf <- test.1 %>% arrange(desc(rf.hint.pred.prob)) %>% 
  mutate(numrank = row_number(), percent.first.action = cumsum(first_action)/numrank,
         method = rep("Random Forest",n())) %>% 
  select(numrank, percent.first.action, method)

plot.data.lm <- test.1 %>% arrange(desc(re.lg.hint.pred.prob)) %>% 
  mutate(numrank = row_number(), percent.first.action = cumsum(first_action)/numrank,
         method = rep("Logistic Regression",n())) %>% 
  select(numrank, percent.first.action,method)

plot.data <- bind_rows(plot.data.rf,plot.data.lm)

##create plot
theme_set(theme_bw())
p <- ggplot(data=plot.data, aes(x=numrank, y=percent.first.action, col = method)) 
p <- p + geom_line()
p <- p + xlab('Number of Attempts') + xlim(1,1000)
p <- p + scale_y_continuous("Percent of Attempts with Hint", limits=c(0.5,1), labels=scales::percent)
p
```
Overall, logistic regression shows a better performance than that of the random forest model.
# Part C: Bottom-up-Hint-Taking Prediction
```{r}
#fit a logistic regression model on train
lg_bottom_hint <- glm(bottom_hint ~ tutor_mode + answer_type + prev_total_correct + 
                 prev_percent_correct + prev_problem_requested_hint + 
                 prev_percent_problem_requested_hint + prev_avg_response_time + 
                 prev_total_bottom_hint + prev_percent_bottom_hint + prev_avg_attempt_count, 
               data=train, family="binomial")
#compute AUC of this model on the test dataset  
test$lg.bottom.hint.pred.prob <- predict(lg_bottom_hint,newdata=test,type='response')
test.lg.bottom.hint.pred <- prediction(test$lg.bottom.hint.pred.prob, test$bottom_hint)
test.lg.bottom.hint.perf <- performance(test.lg.bottom.hint.pred, "auc")
cat('the auc score is ', 100*test.lg.bottom.hint.perf@y.values[[1]], "\n")  
#calculate the five smallest and five largest coefficients 
lg_bottom_hint_coef <- summary(lg_bottom_hint)[["coefficients"]]
lg_bottom_hint_coef[order(lg_bottom_hint_coef[ , 1]), ]  

```

The AUC of the logistic model predicting bottom_hint on the test dataset is 83.22287

The five smallest and five largest coefficients in this model

Smallest:
1. answer_typechoose_1
2. answer_typechoose_n
3. prev_total_correct
4. prev_avg_response_time
5. prev_problem_requested_hint

Largest:
1. tutor_modetutor
2. prev_percent_bottom_hint
3. answer_typefill_in_1
4. prev_avg_attempt_count
5. prev_percent_problem_requested_hint

```{r}
# forward stepwise selection
regfit.fwd = regsubsets(bottom_hint ~ prev_total_correct +
                          prev_percent_correct + prev_problem_requested_hint + 
                          prev_percent_problem_requested_hint + prev_avg_response_time + 
                          prev_total_bottom_hint + prev_percent_bottom_hint +
                          prev_avg_attempt_count, data=train, nvmax=8, method="forward")

summary(regfit.fwd)

# find optimal model

test.mat = model.matrix(bottom_hint ~ prev_total_correct +
                          prev_percent_correct + prev_problem_requested_hint + 
                          prev_percent_problem_requested_hint + prev_avg_response_time + 
                          prev_total_bottom_hint + prev_percent_bottom_hint +
                          prev_avg_attempt_count, data=test, nvmax=8)

val.errors=rep(NA,8)
for (i in 1:8) {
  coefi = coef(regfit.fwd, id=1)
  pred=test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean((as.numeric(test$bottom_hint)-pred)^2)
}

```

```{r}
# refit the model with less features by dropping prev_total_correct, prev_avg_response_time, prev_problem_requested_hint

##Drop the predictors and refit the logistic model 
```{r}
# refit the model based on the results from forward stepwise selection by including 6 features
test.mat = model.matrix(bottom_hint ~ prev_total_correct +
                          prev_percent_correct + prev_problem_requested_hint + 
                          prev_percent_problem_requested_hint + prev_avg_response_time + 
                          prev_total_bottom_hint + prev_percent_bottom_hint +
                          prev_avg_attempt_count, data=test, nvmax=8)

val.errors=rep(NA,8)
for (i in 1:8) {
  coefi = coef(regfit.fwd, id=1)
  pred=test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean((as.numeric(test$bottom_hint)-pred)^2)
}

```


```{r}
# refit the model with less features by dropping prev_total_correct, prev_avg_response_time, prev_problem_requested_hint


re_lg_bottom_hint <- glm(bottom_hint ~ tutor_mode + answer_type + 
                 prev_percent_correct + 
                 prev_percent_problem_requested_hint + 
                 prev_total_bottom_hint + prev_percent_bottom_hint + prev_avg_attempt_count, 
               data=train, family="binomial")

#compute AUC of this model on the test dataset  
test$re.lg.bottom.hint.pred.prob <- predict(re_lg_bottom_hint,newdata=test,type='response')
test.re.lg.bottom.hint.pred <- prediction(test$re.lg.bottom.hint.pred.prob, test$first_action)
test.re.lg.bottom.hint.perf <- performance(test.re.lg.bottom.hint.pred, "auc")
cat('the auc score is ', 100*test.re.lg.bottom.hint.perf@y.values[[1]], "\n")  
#calculate the five smallest and five largest coefficients 
re_lg_bottom_hint_coef <- summary(re_lg_bottom_hint)[["coefficients"]]
re_lg_bottom_hint_coef[order(re_lg_bottom_hint_coef[ , 1]), ]  

```
The AUC of the revised logistic model predicting bottom_hint on the test dataset is 81.26528

The smallest and five largest coefficients in this model

Smallest:
1. answer_typechoose_1
2. answer_typechoose_n
3. prev_percent_correct
4. prev_total_bottom_hint

Largest:
1. tutor_modetutor
2. prev_percent_bottom_hint
3. answer_typefill_in_1
4. prev_percent_problem_requested_hint
5. prev_avg_attempt_count 

# Part D: Hint-Taking Prediction
```{r}
#fit a random forest model on train
rf_bottom_hint <- randomForest(bottom_hint ~ tutor_mode + answer_type + 
                 prev_percent_correct + 
                 prev_percent_problem_requested_hint + 
                 prev_total_bottom_hint + prev_percent_bottom_hint + prev_avg_attempt_count, 
               data=train, ntree=200, na.action=na.omit)

#compute AUC of this model on the test dataset  
test$rf.bottom.hint.pred.prob <- predict(rf_bottom_hint,newdata=test,type='prob')[,2]
test.rf.bottom.hint.pred <- prediction(test$rf.bottom.hint.pred.prob, test$first_action)
test.rf.bottom.hint.perf <- performance(test.rf.bottom.hint.pred, "auc")
cat('the auc score is ', 100*test.rf.bottom.hint.perf@y.values[[1]], "\n") 

```

# Part D: Hint-Taking Prediction
```{r}
```

# Part #: Scaffolding-Taking Prediction
** Right now, I don't want to predict scaffolding. This is too complicated to consider since if a problem has scaffolding and the student answers incorrectly or asks for the problem to be broken into steps, a new problem will be created called a scaffolding problem. LMK