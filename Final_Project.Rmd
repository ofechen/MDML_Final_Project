---
title: "MDML_Final_Project_YJ_FT_OC"
output: html_notebook
author: Yeonji Jung, Frankie Tam, Ofer Chen
---
#Description

##BACKGROUND
Hints are an important learning aid. But students may have a tendency to use hints inappropriately. 
Offering hints indiscriminately can result in poor learning performance. A prediction model on student hint-taking behaviors can be used to make adaptive decisions on whether to withhold or provide hints. Such model can be useful for teachers trying decide whether to show a hint or scaffolding to students in the further iterations.

##DATA 
The dataset is taken from the ASSISTments' skill builder problem. The data set was gathered in the school year 2009/10. 
ASSISTments is an online tutoring system that teachers can use to teach middle-school level Mathematics and identify student activities solving exercises on the system. 

The dataset consists of log data of student activities including the chronological order of attempts (order_id), assignment id, problem id, user id, whether the first attempt is correct (correct), whether the number of attempts on the problem (attempt_count), the number of using hints on the problem (hint_count), whether or not the student asks for all hints (bottom_hint), the time in milliseconds for the student to complete the problem (overlap_time), time between start time and first student action (first_response_time), whether the first action is attempt, using hint, or using scaffolding (first_action). Each row represents a single problem of each student.

The dataset has the following features:
(a) Questions are based on one specific skill, a question can have multiple skill taggings 
(b) Students must answer three questions correct in a row to complete the assignment
(c) If a student uses the tutoring aids ("hint" or "scaffolding"), the question will be marked incorrect
(d) Students will know immediately if they answered the question correctly
(e) The scaffolding option will provide the student a break-down of the problem into small steps
(f) The bottom out hint is the last hint for a problem and will generally contain the problemâ€™s answer

The raw dataset is available at:
https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-data/skill-builder-data-2009-2010

##PURPOSE
Building a prediction model on student actions on next question, possible options are:
(a) whether students will take a hint at first attempt (first_action)
(b) whether students will take a bottom-out hint (bottom_hint)
(c) whether students will answer correctly without hints (correct)

##APPROACH 
We'll build three different models predicting student actions on next question: (a) hint-taking, (b) bottom-out hint-taking, (c) correct answers. For each part, we select featuers, split train and test set, fit two models of logistic regression and random forest, test it with test dataset, and choose between 2 models calculating AUC. (why you think this is a reasonable approach to take). 


#Part A: Import and Clean the Data
##Import packages and data
```{r}
#load packages
library(tidyverse)
library(randomForest)
library(doParallel)
library(foreach)
library(car)
library(ROCR)
library(leaps)

#import the data
df_raw <- read.csv("skill_builder_data.csv")
```

##Clean the data
```{r}
#filter the following values
##("original") scaffolding problems (0); this is not relevant to this project
##("first_action") empty values; student clicked on the problem but did nothing else
##("first_action") scaffolding values; scaffolding opportunities varied across problem sets
##("answer_type") open_response; this type of answers always marked correct
##("ms_first_response") negative values and zero
##("overlap_time") negative values and zero

df_clean <- df_raw %>% 
  filter(original==1, first_action!=2, !is.na(first_action), 
         answer_type!="open_response", ms_first_response > 0, overlap_time > 0) %>%
  rename(problem_type = type)

#revalue the following columns:
##("bottom_hint") replace empty values into 0; it means the student did not ask for a hint
df_clean$bottom_hint[is.na(df_clean$bottom_hint)] <- 0
```

#Part B: Hint-Taking Prediction
##Feature engineering; make historical features
```{r}
#use the latest order id as reference
df_last <- df_clean %>% group_by(user_id) %>% arrange(order_id) %>% slice(n())

df_hist <- merge(df_last, df_clean[c('user_id', 'order_id', 'problem_id', 'correct', 'hint_count', 'hint_total', 'bottom_hint', 'ms_first_response', 'attempt_count')], by = 'user_id', all.x = TRUE, allow.cartesian=TRUE)

df_hist <- df_hist %>% filter(order_id.y < order_id.x)

df_hist <- df_hist %>% group_by(user_id, order_id.x) %>% 
  summarise(prev_total_correct = sum(correct.y),
            prev_num_problem = length(unique(problem_id.y)),
            prev_percent_correct=prev_total_correct/prev_num_problem,
            prev_questions_with_hints = sum(hint_total.y>0),
            prev_total_hints = sum(hint_total.y),
            prev_avg_hints_per_problem = prev_total_hints/prev_questions_with_hints,
            prev_problem_requested_hint = sum(hint_count.y>0),
            prev_percent_problem_requested_hint = prev_problem_requested_hint/prev_num_problem,
            prev_avg_response_time = mean(ms_first_response.y),
            prev_total_bottom_hint = sum(bottom_hint.y == 1),
            prev_percent_bottom_hint = prev_total_bottom_hint/prev_questions_with_hints,
            prev_avg_attempt_count = mean(attempt_count.y)) 

df_hist <- replace(df_hist, is.na(df_hist), 0)

df_fin <-  df_clean %>% left_join(df_hist)

#convert to factors
df_fin <- df_fin %>%
  mutate_at(vars(first_action,correct,bottom_hint,problem_id,teacher_id, school_id),funs(as.factor))
```

##Create a training Set / test Set
```{r}
#randomly shuffle the data
set.seed(1314)
df_fin <- df_fin %>% slice(sample(1:n()))

#create train_half and test_half
splitsize = floor(nrow(df_fin)/2)
train <- df_fin %>% slice(1:splitsize)
test <- df_fin %>% slice(splitsize+1:n())
```

##Fit a logistic model on train and compute AUC on test set
```{r}
#fit a logistic regression model on train
lg_hint <- glm(first_action ~ tutor_mode + answer_type + prev_total_correct + 
                 prev_percent_correct + prev_problem_requested_hint + 
                 prev_percent_problem_requested_hint + prev_avg_response_time + 
                 prev_total_bottom_hint + prev_percent_bottom_hint + prev_avg_attempt_count, 
               data=train, family="binomial")
#compute AUC of this model on the test dataset  
test$lg.hint.pred.prob <- predict(lg_hint,newdata=test,type='response')
test.lg.hint.pred <- prediction(test$lg.hint.pred.prob, test$first_action)
test.lg.hint.perf <- performance(test.lg.hint.pred, "auc")
cat('the auc score is ', 100*test.lg.hint.perf@y.values[[1]], "\n")  
#calculate the five smallest and five largest coefficients 
lg_hint_coef <- summary(lg_hint)[["coefficients"]]
lg_hint_coef[order(lg_hint_coef[ , 1]), ]   
```
Before fitting a model, I dropped some features (which I hoped to embed in the model); 
  problem_type = only 1 factor 
  problem_id = more than 53 factors
  teacher_id = more than 53 factors
  
The AUC of the logistic model predicting hint-taking on the test dataset is 84.2032

The five smallest and five largest coefficients in this model
  Smallest: answer_typechoose_1 < answer_typechoose_n < prev_percent_correct <
            prev_total_bottom_hint < prev_total_correct 
  Largest:  tutor_modetutor > prev_percent_bottom_hint > answer_typefill_in_1 > 
            prev_percent_problem_requested_hint > prev_problem_requested_hint 
  
The features I decided to drop: 
  (a) prev_problem_requested_hint, (b) prev_percent_correct, (c) prev_total_bottom_hint
  These features had similar in the contents respectively with 
  (a1) prev_percent_problem_requested_hint, (b1) prev_total_correct, and (c1) prev_percent_bottom_hint, 
  and each had relatively low coefficient than its compared feature. 
  
  
```{r}
# forward stepwise selection
# error if includes anwser_type
regfit.fa.fwd = regsubsets(first_action ~ tutor_mode + prev_total_correct +
                          prev_percent_correct + prev_problem_requested_hint + 
                          prev_percent_problem_requested_hint + prev_avg_response_time + 
                          prev_total_bottom_hint + prev_percent_bottom_hint +
                          prev_avg_attempt_count, data=train, method="forward")

reg_fa_summary = summary(regfit.fa.fwd)
reg_fa_summary
```

```{r}
# plots to examine RSS, R2, Cp and BIC 
{plot(reg_fa_summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg_fa_summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")

adj_r2_max = which.max(reg_fa_summary$adjr2)

points(adj_r2_max, reg_fa_summary$adjr2[adj_r2_max], col ="red", cex = 2, pch = 20)

plot(reg_fa_summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp_min = which.min(reg_fa_summary$cp) # 10
points(cp_min, reg_fa_summary$cp[cp_min], col = "red", cex = 2, pch = 20)

plot(reg_fa_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(reg_fa_summary$bic) # 6
points(bic_min, reg_fa_summary$bic[bic_min], col = "red", cex = 2, pch = 20)}


```
  
##Drop the predictors and refit the logistic model 
```{r}
#refit a logistic regression model on train using 4 features based on forward stepwise selection

# prev_problem_requested_hint 
# prev_percent_problem_requested_hint
# prev_total_bottom_hint 
# prev_percent_bottom_hint

# re_lg_hint <- glm(first_action ~ tutor_mode + answer_type + prev_total_correct + 
#                  prev_percent_problem_requested_hint + prev_avg_response_time + 
#                  prev_percent_bottom_hint + prev_avg_attempt_count, 
#                data=train, family="binomial")

re_lg_hint <- glm(first_action ~ prev_problem_requested_hint +
                 prev_percent_problem_requested_hint + prev_total_bottom_hint +
                 prev_percent_bottom_hint,
               data=train, family="binomial")

#compute AUC of this model on the test dataset  
test$re.lg.hint.pred.prob <- predict(re_lg_hint,newdata=test,type='response')
test.re.lg.hint.pred <- prediction(test$re.lg.hint.pred.prob, test$first_action)
test.re.lg.hint.perf <- performance(test.re.lg.hint.pred, "auc")
cat('the auc score is ', 100*test.re.lg.hint.perf@y.values[[1]], "\n")  
#calculate the five smallest and five largest coefficients 
re_lg_hint_coef <- summary(re_lg_hint)[["coefficients"]]
re_lg_hint_coef[order(re_lg_hint_coef[ , 1]), ]   
```

The AUC of the logistic model predicting hint-taking on the test dataset is 82.84269 (*decreasing)
The smallest and five largest coefficients in this model
  Smallest: answer_typechoose_1 < answer_typechoose_n < prev_percent_problem_requested_hint < 
            prev_total_correct 
  Largest:  tutor_modetutor > prev_percent_bottom_hint > answer_typefill_in_1 > 
            prev_avg_attempt_count > prev_avg_response_time
            
```{r}
#fit a random forest model on train
# rf_hint <- randomForest(first_action ~ tutor_mode + answer_type + prev_total_correct + 
#                  prev_percent_problem_requested_hint + prev_avg_response_time + 
#                  prev_percent_bottom_hint + prev_avg_attempt_count, data=train, ntree=200, 
#                  na.action=na.omit)

rf_hint <- randomForest(first_action ~ prev_problem_requested_hint +
                 prev_percent_problem_requested_hint + prev_total_bottom_hint +
                 prev_percent_bottom_hint,
               data=train, ntree = 200, na.action=na.omit)

#compute AUC of this model on the test dataset  
test$rf.hint.pred.prob <- predict(rf_hint,newdata=test,type='prob')[,2]
test.rf.hint.pred <- prediction(test$rf.hint.pred.prob, test$first_action)
test.rf.hint.perf <- performance(test.rf.hint.pred, "auc")
cat('the auc score is ', 100*test.rf.hint.perf@y.values[[1]], "\n") 
```
The computed auc score is 70.20511. *How does the AUC of the random forest compare with the AUC of the logistic regression model?
```{r}
#compares the performance of two models using the plots
##create a performance plot
test.1 <- test %>% mutate(first_action = as.integer(first_action)-1)
plot.data.rf <- test.1 %>% arrange(desc(rf.hint.pred.prob)) %>% 
  mutate(numrank = row_number(), percent.first.action = cumsum(first_action)/numrank,
         method = rep("Random Forest",n())) %>% 
  select(numrank, percent.first.action, method)
plot.data.lm <- test.1 %>% arrange(desc(re.lg.hint.pred.prob)) %>% 
  mutate(numrank = row_number(), percent.first.action = cumsum(first_action)/numrank,
         method = rep("Logistic Regression",n())) %>% 
  select(numrank, percent.first.action,method)
plot.data <- bind_rows(plot.data.rf,plot.data.lm)
##create plot
theme_set(theme_bw())
p <- ggplot(data=plot.data, aes(x=numrank, y=percent.first.action, col = method)) 
p <- p + geom_line()
p <- p + xlab('Number of Attempts') + xlim(1,1000)
p <- p + scale_y_continuous("Percent of Attempts with Hint", limits=c(0.5,1), labels=scales::percent)
p
```
Overall, logistic regression shows a better performance than that of the random forest model.

# Part C: Bottom-up-Hint-Taking Prediction
```{r}
#fit a logistic regression model on train
lg_bottom_hint <- glm(bottom_hint ~ tutor_mode + answer_type + prev_total_correct +
                 prev_percent_correct + prev_problem_requested_hint +
                 prev_percent_problem_requested_hint + prev_avg_response_time +
                 prev_total_bottom_hint + prev_percent_bottom_hint + prev_avg_attempt_count,
               data=train, family="binomial")

#compute AUC of this model on the test dataset  
test$lg.bottom.hint.pred.prob <- predict(lg_bottom_hint,newdata=test,type='response')
test.lg.bottom.hint.pred <- prediction(test$lg.bottom.hint.pred.prob, test$bottom_hint)
test.lg.bottom.hint.perf <- performance(test.lg.bottom.hint.pred, "auc")
cat('the auc score is ', 100*test.lg.bottom.hint.perf@y.values[[1]], "\n")  
#calculate the five smallest and five largest coefficients 
lg_bottom_hint_coef <- summary(lg_bottom_hint)[["coefficients"]]
lg_bottom_hint_coef[order(lg_bottom_hint_coef[ , 1]), ]  

```

The AUC of the logistic model predicting bottom_hint on the test dataset is 83.22287

The five smallest and five largest coefficients in this model

Smallest:
1. answer_typechoose_1
2. answer_typechoose_n
3. prev_total_correct
4. prev_avg_response_time
5. prev_problem_requested_hint

Largest:
1. tutor_modetutor
2. prev_percent_bottom_hint
3. answer_typefill_in_1
4. prev_avg_attempt_count
5. prev_percent_problem_requested_hint

```{r}
# forward stepwise selection
# error if includes anwser_type
regfit.bh.fwd = regsubsets(bottom_hint ~ tutor_mode + prev_total_correct +
                          prev_percent_correct + prev_problem_requested_hint + 
                          prev_percent_problem_requested_hint + prev_avg_response_time + 
                          prev_total_bottom_hint + prev_percent_bottom_hint +
                          prev_avg_attempt_count, data=train, method="forward")

reg_bh_summary = summary(regfit.bh.fwd)
reg_bh_summary
```

```{r}
# code from textbook but couldn't get it to work yet
# test.mat = model.matrix(bottom_hint ~ tutor_mode + prev_total_correct +
#                           prev_percent_correct + prev_problem_requested_hint + 
#                           prev_percent_problem_requested_hint + prev_avg_response_time + 
#                           prev_total_bottom_hint + prev_percent_bottom_hint +
#                           prev_avg_attempt_count, data=test)
# 
# val.errors=rep(NA,6)
# for (i in 1:6){
#   coefi=coef(regfit.fwd, id=i)
#   pred=test.mat[,names(coefi)]%*%coefi
#   val.errors[i]=mean((as.integer(test$bottom_hint)-pred)^2)
# }
# 
# val.errors
# 
# which.min(val.errors)

# plots to examine RSS, R2, Cp and BIC 
{plot(reg_bh_summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg_bh_summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")

adj_r2_max = which.max(reg_bh_summary$adjr2)

points(adj_r2_max, reg_bh_summary$adjr2[adj_r2_max], col ="red", cex = 2, pch = 20)

plot(reg_bh_summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp_min = which.min(reg_bh_summary$cp) # 10
points(cp_min, reg_bh_summary$cp[cp_min], col = "red", cex = 2, pch = 20)

plot(reg_bh_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(reg_bh_summary$bic) # 6
points(bic_min, reg_bh_summary$bic[bic_min], col = "red", cex = 2, pch = 20)}
```


##Drop the predictors and refit the logistic model 
```{r}
# refit the model based on the results from forward stepwise selection by including 6 features

re_lg_bottom_hint <- glm(bottom_hint ~ prev_percent_correct +
                           prev_total_correct +
                           prev_problem_requested_hint +
                           prev_percent_problem_requested_hint +
                           prev_total_bottom_hint + prev_percent_bottom_hint,
                          data=train, family="binomial")

#compute AUC of this model on the test dataset  
test$re.lg.bottom.hint.pred.prob <- predict(re_lg_bottom_hint,newdata=test,type='response')
test.re.lg.bottom.hint.pred <- prediction(test$re.lg.bottom.hint.pred.prob, test$bottom_hint)
test.re.lg.bottom.hint.perf <- performance(test.re.lg.bottom.hint.pred, "auc")
cat('the auc score is ', 100*test.re.lg.bottom.hint.perf@y.values[[1]], "\n")  
#calculate the five smallest and five largest coefficients 
re_lg_bottom_hint_coef <- summary(re_lg_bottom_hint)[["coefficients"]]
re_lg_bottom_hint_coef[order(re_lg_bottom_hint_coef[ , 1]), ]  

```
The AUC score of the revised logistic model for predicting bottom_hint on the test dataset is 82.30289

The smallest andlargest coefficients in this model

Smallest: prev_total_correct

Largest: prev_percent_bottom_hint

```{r}
#fit a random forest model on train
rf_bottom_hint <- randomForest(bottom_hint ~ prev_percent_correct +
                           prev_total_correct +
                           prev_problem_requested_hint + 
                           prev_percent_problem_requested_hint + 
                           prev_total_bottom_hint + prev_percent_bottom_hint, 
               data=train, ntree=200, na.action=na.omit)

#compute AUC of this model on the test dataset  
test$rf.bottom.hint.pred.prob <- predict(rf_bottom_hint,newdata=test,type='prob')[,2]
test.rf.bottom.hint.pred <- prediction(test$rf.bottom.hint.pred.prob, test$bottom_hint)
test.rf.bottom.hint.perf <- performance(test.rf.bottom.hint.pred, "auc")
cat('the auc score is ', 100*test.rf.bottom.hint.perf@y.values[[1]], "\n") 

```
The AUC score of randomforest model for predicting bottom_hint on the test dataset is 61.53358


```{r}
#compares the performance of two models using the plots
##create a performance plot
test.bh <- test %>% mutate(bottom_hint = as.integer(bottom_hint)-1)
plot.data.rf.bh <- test.bh %>% arrange(desc(rf.bottom.hint.pred.prob)) %>% 
  mutate(numrank = row_number(), percent.bottom.hint = cumsum(bottom_hint)/numrank,
         method = rep("Random Forest",n())) %>% 
  select(numrank, percent.bottom.hint, method)
plot.data.lm.bh <- test.bh %>% arrange(desc(re.lg.bottom.hint.pred.prob)) %>% 
  mutate(numrank = row_number(), percent.bottom.hint = cumsum(bottom_hint)/numrank,
         method = rep("Logistic Regression",n())) %>% 
  select(numrank, percent.bottom.hint,method)
plot.data.bh <- bind_rows(plot.data.rf.bh,plot.data.lm.bh)
##create plot
theme_set(theme_bw())
p <- ggplot(data=plot.data.bh, aes(x=numrank, y=percent.bottom.hint, col = method)) 
p <- p + geom_line()
p <- p + xlab('Number of Attempts') + xlim(1,1000)
p <- p + scale_y_continuous("Percent of Using All Hints", limits=c(0.5,1), labels=scales::percent)
p
```

# Part D: Answer correctly without hint Prediction
```{r}
#fit a logistic regression model on train
lg_correct <- glm(correct ~ tutor_mode + answer_type + prev_total_correct +
                 prev_percent_correct + prev_problem_requested_hint +
                 prev_percent_problem_requested_hint + prev_avg_response_time +
                 prev_total_bottom_hint + prev_percent_bottom_hint + prev_avg_attempt_count,
               data=train, family="binomial")

#compute AUC of this model on the test dataset  
test$lg.correct.pred.prob <- predict(lg_correct,newdata=test,type='response')
test.lg.correct.pred <- prediction(test$lg.correct.pred.prob, test$correct)
test.lg.correct.perf <- performance(test.lg.correct.pred, "auc")
cat('the auc score is ', 100*test.lg.correct.perf@y.values[[1]], "\n")  
#calculate the five smallest and five largest coefficients 
lg_correct_coef <- summary(lg_correct)[["coefficients"]]
lg_correct_coef[order(lg_correct_coef[ , 1]), ]  
```


```{r}
# forward stepwise selection
# error if includes anwser_type
regfit.c.fwd = regsubsets(correct ~ tutor_mode + prev_total_correct +
                          prev_percent_correct + prev_problem_requested_hint + 
                          prev_percent_problem_requested_hint + prev_avg_response_time + 
                          prev_total_bottom_hint + prev_percent_bottom_hint +
                          prev_avg_attempt_count, data=train, method="forward")

reg_c_summary = summary(regfit.c.fwd)
reg_c_summary

```
```{r}
{plot(reg_c_summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg_c_summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")

adj_r2_max = which.max(reg_c_summary$adjr2)

points(adj_r2_max, reg_c_summary$adjr2[adj_r2_max], col ="red", cex = 2, pch = 20)

plot(reg_c_summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp_min = which.min(reg_c_summary$cp) # 10
points(cp_min, reg_c_summary$cp[cp_min], col = "red", cex = 2, pch = 20)

plot(reg_c_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(reg_c_summary$bic) # 6
points(bic_min, reg_c_summary$bic[bic_min], col = "red", cex = 2, pch = 20)}
```
```{r}
# refit the model based on the results from forward stepwise selection by including 5 features

re_lg_correct <- glm(correct ~ prev_percent_correct +
                           prev_total_correct +
                           prev_percent_problem_requested_hint +
                           prev_avg_attempt_count +
                           prev_percent_bottom_hint,
                          data=train, family="binomial")

#compute AUC of this model on the test dataset  
test$re.lg.correct.pred.prob <- predict(re_lg_correct,newdata=test,type='response')
test.re.lg.correct.pred <- prediction(test$re.lg.correct.pred.prob, test$correct)
test.re.lg.correct.perf <- performance(test.re.lg.correct.pred, "auc")
cat('the auc score is ', 100*test.re.lg.correct.perf@y.values[[1]], "\n")  
#calculate the five smallest and five largest coefficients 
re_lg_correct_coef <- summary(re_lg_correct)[["coefficients"]]
re_lg_correct_coef[order(re_lg_correct_coef[ , 1]), ]  
```

```{r}
#fit a random forest model on train
rf_correct <- randomForest(correct ~ prev_percent_correct +
                           prev_total_correct +
                           prev_percent_problem_requested_hint +
                           prev_avg_attempt_count +
                           prev_percent_bottom_hint,
               data=train, ntree=200, na.action=na.omit)

#compute AUC of this model on the test dataset  
test$rf.correct.pred.prob <- predict(rf_correct,newdata=test,type='prob')[,2]
test.rf.correct.pred <- prediction(test$rf.correct.pred.prob, test$correct)
test.rf.correct.perf <- performance(test.rf.correct.pred, "auc")
cat('the auc score is ', 100*test.rf.correct.perf@y.values[[1]], "\n") 

```
```{r}
#compares the performance of two models using the plots
##create a performance plot
test.correct <- test %>% mutate(correct = as.integer(correct)-1)
plot.data.rf.correct <- test.correct %>% arrange(desc(rf.correct.pred.prob)) %>% 
  mutate(numrank = row_number(), percent.correct = cumsum(correct)/numrank,
         method = rep("Random Forest",n())) %>% 
  select(numrank, percent.correct, method)
plot.data.lm.correct <- test.correct %>% arrange(desc(re.lg.correct.pred.prob)) %>% 
  mutate(numrank = row_number(), percent.correct = cumsum(correct)/numrank,
         method = rep("Logistic Regression",n())) %>% 
  select(numrank, percent.correct,method)
plot.data.correct <- bind_rows(plot.data.rf.correct,plot.data.lm.correct)
##create plot
theme_set(theme_bw())
p <- ggplot(data=plot.data.correct, aes(x=numrank, y=percent.correct, col = method)) 
p <- p + geom_line()
p <- p + xlab('Number of Attempts') + xlim(1,1000)
p <- p + scale_y_continuous("Percent of correct answers", limits=c(0.5,1), labels=scales::percent)
p
```

# Part #: Scaffolding-Taking Prediction
** Right now, I don't want to predict scaffolding. This is too complicated to consider since if a problem has scaffolding and the student answers incorrectly or asks for the problem to be broken into steps, a new problem will be created called a scaffolding problem. LMK
